![[hyena.assets/Pasted image 20240610195712.png]]
# 鬣(Liè)狗层次结构：迈向更大的卷积语言模型

## 摘要：
Transformer的注意力模块对于序列长度有二次成本，限制了上下文的访问量。现有的基于低秩和稀疏近似的次二次方法需要与密集注意力层相结合才能匹配 Transformer，这表现了能力上的差距。

本文提出了Hyena，一个次二次的注意力替代模块，通过交错地使用隐式参数化的**长卷积**和**数据控制门控**构建。在处理数千到数十万个标记的回归和推理任务中，Hyena在准确性上比依赖于状态空间和其他隐式和显式方法的操作符提高了超过50个百分点，与基于注意力的模型相当。

在语言建模标准数据集（WIKITEXT103和THE PILE）上为不需要密集注意力的架构设定了新的最优结果，以序列长度 2K为例，训练计算要求减少了20%。在序列长度为8K 和 64K 时，hynea运算速度是高度优化的注意力模型的两倍和 100 倍。

---

## 1.引言
大型Transformer已经在语言建模、视觉、音频、生物学和许多其他领域取得了一系列突破性进展。他的成功很大程度上依赖于缩放特性和上下文学习的出现，使其能够以上下文作为输入来推广到未见过的数据和任务。

> 缩放特性：要实现最佳计算训练，模型大小和训练词库数量应等比例缩放：模型大小每增加一倍，训练词库数量也应增加一倍。**Training Compute-Optimal Large Language Models，DeepMind**
> 上下文学习：大型语言模型表现出一定的执行上下文学习的能力，但目前尚不清楚成功的任务与训练数据中存在的内容之间的关系是什么。标准Transformer可以能够从上下文示例中学习看不见的线性函数，其性能可与最优最小二乘估计器相媲美。即使在两种形式的分布偏移下，上下文学习也是可能的：（i）在模型的训练数据和推理时提示之间，以及（ii）在推理过程中的上下文示例和查询输入之间。训练 Transformer 可以在上下文中学习更复杂的函数类——即稀疏线性函数、两层神经网络和决策树——其性能与特定于任务的学习算法相匹配或超过。**What can transformers learn in-context? a case study of simple function classes**

但它也有局限性，最值得注意的问题之一是计算成本，随着输入序列长度的增加而迅速增长。该成本与序列长度L的平方成比例增加，这对于模型可以考虑的上下文量设定了严格限制。
![[hyena.assets/Pasted image 20240618180100.png|450]]

突破二次限制是迈向深度学习的新可能性的关键步骤。

减少模型中注意力计算成本的努力主要涉及线性化、低秩和稀疏逼近的使用。这些方法在表达能力和速度之间引入了一种折衷，需要与标准的注意力层相结合才能达到Transformer的质量.

越来越多的证据表明，注意力机制在语言处理中只利用了其二次能力的一小部分。

>1. **Hungry Hungry Hippos: Towards Language Modeling with State Space Models**
>H3模型通过将两层SSM堆叠，结合输入和输出的乘法项，在很多情况下表现得非常接近甚至优于Transformer。同时，H3模型由于其线性复杂度，相比于Transformer在长序列处理上具有显著的计算优势。通过引入FlashConv算法以及针对长序列的优化，SSM能够实现与注意力机制相媲美的性能，而且计算效率更高。
>2. **In-context learning and induction heads**
>论据 1（宏观共同认知）： transformer语言模型在训练初期会经历一个 "阶段性变化"，在此期间会形成induction heads，同时上下文学习能力也会显著提高。
>论据 2（宏观共扰）： 当我们改变 transformer的结构，从而改变induction heads能否形成（以及何时形成）时，语境中学习的显著提高也会发生精确匹配的变化。
>论据 3（直接消融）：  当我们在测试时直接 "消除 "小模型中的induction heads时，上下文中学习的数量会大大减少。
>论据 4（induction heads通用性的具体例子）： 尽管我们从复制文字序列的角度对induction heads进行了非常狭义的定义，但我们根据经验观察到，这些induction heads似乎还能实现更复杂类型的情境内学习，包括高度抽象的行为，因此它们可以解释大部分的情境内学习。
>论据5（induction head通用性的机制合理性）： 对于小型模型，我们可以从机制上解释induction heads是如何工作的，并能证明它们有助于情境中学习。此外，实际的运作机制表明，我们可以通过自然的方式重新利用它来进行更普遍的情境学习。
>论证 6（从小模型到大模型的连续性）： 在前5个论点中，induction heads解释情境学习的理由在小模型中比在大模型中更充分。然而，许多与induction heads和情境学习相关的行为和数据从小型模型到大型模型都是平滑连续的，这表明最简单的解释是机制是相同的。